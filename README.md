# 100DaysofMLCode
On this repository I will be updating all the stuff that I do every day related to Machine Learning for the 100DaysofMLCode Challenge proposed by Siraj Raval.


## DAY 1 : 
So this is the first day of #100DAYSOFMLCODE . I am so excited for the next 100 days that I am going to spend learning and using machine learning. 
Today was a great day. I attended the Chennai School of AI meetup and got to learn a lot of interesting things and got to know a lot of interesting people. 
We had a very fruitful discussion on applications of ML and AI. Got to know about interesting projects that some really experienced people had worked on. Some of them were --> Automatic quiz creation app, Chatbots, Detecting elephants near railways tracks and stopping them from being hit by trains, Automatic music creation web app(by the Dean himself) and many other interesting things. 
Also got to know about many new machine learning libraries like --> spacy, scikit, tensorflow, etc.
Had a very great day and I was inspired to learn ML and go deep into the field.


## DAY 2: 
Started with the Python for Data Science series by @sirajraval on YouTube. Made a simple gender classification code using weight, height and shoe size as the only inputs.
Made it with scikit-learn (Decision Tree) guided by siraj and further made the same using the scikit-learn KNearestNeighbors method and QuadraticDiscriminantAnalysis method.
It is awesome just how a few lines in Python can create a gender classification code. It felt really awesome writing my first machine learning code. 
#100DaysofMLCode 


## DAY 3 :
Made a Twitter sentiment analysis code using libraries like tweepy and textblob. Followed the tutorial by @sirajraval , which is the second video in his Python for Data Science Series.

Completed the assignment and successfully created a csv file using the pandas library. You will find it soon on my github.

Learnt a bit about natural language processing from this tutorial and found it really interesting. 


## DAY 4 :
Learnt about vector spaces from MIT Open Courseware. Maths plays a very important role in ML. It is necessary to have good knowledge of linear algebra, probability and statistics and calculus.

All these can be valuable while developing your own machine learning algorithms. 

## DAY 5 :
 Made a movie recommendation system by following the third video of @sirajraval 's Python for Data Science series. 
I used three dependencies:
-- numpy
-- scipy
-- lightfm

lightfm is a library which has inbuilt recommendation system algorithms. Used the fetch_movielens function to gather all movie data and used warp as my loss function to improve the model and reduce the loss over time.

Learnt about two types of recommendation systems : --collabrative (similar users data)
--content based (only the users data) It was awesome learning and I was finally happy when I understood the whole code.

#100DaysofMLCode

## DAY 6 :
 Do you want to get rich by investing in stock markets. Well then, Machine Learning can help you understand where to invest so as to get the most profit.

I developed a stock price prediction model by following @sirajraval 's fourth video in his Python for Data Science series.

I used the following dependencies :
- CSV
- numpy
- matplotlib
- sklearn.svm

#100DaysofMLCode

## DAY 7 :
Today I revised Decision Tree Classifiers and made a simple model for predicting the type of iris flower.

I got a better understanding of training and testing data.

I used scikit learn and numpy as the major libraries

#100DaysofMLCode

## DAY 8 :
 Read a paper on the development of Speech Recognition systems while on my journey from Bangalore to Chennai.

Attended a coding competition in Bangalore and spent the whole day coding.

#100DaysofMLCode

## DAY 9 :
Learnt how to find accuracies of Machine Learning Algorithms ( basically Classifiers) using the sklearn.metrics function.

I found the accuracy of Decision Trees and KNeighbors Classifier using the iris dataset inbuilt inside scikitlearn.

For me Kneighborsclassifier turned out to be more accurate than Decision Trees.

#100DaysofMLCode .

## DAY 10 :
Made a small version of the KNeareatNeighbors Classifier. KNN basically works on the principle of Euclidean distance.

I got an accuracy of 96% using this classifier.

Though it is simple to make, this classifier may work slow if you have a huge amount of data. It also doesn't represent relationships between features and therefore doesn't not take everything into consideration.

Learnt this by following Josh Gordon on the @googledevs youtube channel.

#100DaysofMLCode 

## DAY 11 :
Revised what I have learned from the past 10 days.

Got a refresher and overview of the past 10 days.

Moving ahead from tomorrow... #100DaysofMLCode .

## DAY 12 :
 Learnt how to use Linear Regression using scikitlearn.

Applied it for predicting body weight using just the brain weight.

Made this by following @sirajraval 's 1st video in his intro Deep Learning series.

#100DaysofMLCode .


## DAY 13  :
I set up my github and uploaded all my work done till Day 12.

All ML Codes are available as .ipynb file.

Pls do contribute.


## DAY 14 : 
Started to read an online book named Neural Networks and Deep Learning by Michael Nielsen.

Learnt about perceptron -- a type of artificial Neuron.

#100DaysofMLCode .


## DAY 15 : 
Learnt about the Sigmoid Neuron and how it is similar to a perceptron.

A sigmoid Neuron is also known as a logistic Neuron and is another type of an artificial neuron, which has the self learning functionality.

#100DaysofMLCode .

## DAY 16 :
Continuing with the book -- Neural Networks and Deep Learning by Michael Nielsen. 

#100DaysofMLCode .

## DAY 17 :
Tried to make my first Neural Network by following @sirajraval 's deep learning series.

There were some errors. 
Will check them out tomorrow.

#100DaysofMLCode .

## DAY 18 :
Got my first neural network working by following @sirajraval 's second video in his Intro to Deep Learning Series.

Corrected the errors from yesterday's code.

Find the code to my first Neural Net on my GitHub (link in bio). 

#100DaysofMLCode .

## DAY 19 :
Started learning how to classify handwritten digits using a feed forward neural network by following the book ' Neural Networks and deep learning ' by Michael Nielsen.

Learnt how feed forward neural nets are better than recurrent neural nets.

Understood the basic neural network for classifying handwritten digits.

#100DaysofMLCode .

## DAY 20 :
Started learning how to make a simple Neural Network to classify handwritten digits. 
Basically the neural network will have 3 layers --> 1 input, 1 hidden and 1 output.

The weights and biases of the layers will be decided using the gradient descent algorithm.

The cost function will determine the loss or accuracy of the neural networks. 

#100DaysofMLCode .

## DAY 21 :
Moving ahead with gradient descent.....
.
.
.
#100DaysofMLCode .

## DAY 22 :

Finally starting with the code to recognise handwritten digits. I will be using MNIST dataset with 60,000 images as training data and 10,000 images as testing data.

I will use Gradient descent to get the weights and biases for my Neural Network.

Hope to complete the code by tomorrow.

#100DaysofMLCode .

## DAY 23 :
The Handwritten Digits Recognition code does not seem to work so easily.

But I hope to make it work soon.

#100DaysofMLCode 

## DAY 24 :
Got the handwritten digits recognition code working by following the book neural networks and deep learning by Michael Nielsen.

Did the same using tensorflow by following @sirajraval 's tutorial.

Got more than 90% accuracy for both the codes. 
Unfortunately I couldn't make it for accepting user input and predicting the digit. But I will be working on it.

For now it trains on the training data and compares the predicted result with the test data and gives the accuracy.

#100DaysofMLCode 

## DAY 25 :
Started with the Intro to Machine Learning Course by @udacity as suggested by @sirajraval in his Machine Learning in 3 months video.

Learnt the basics and the first two lessons were a revision for me. The were all about supervised and unsupervised learning.

Learnt about Naive Bayes implementation using the scikit-learn library.

Hope to learn a lot from this course.

#100DaysofMLCode 

## DAY 26 :
Continuing with @udacity 's course.... #100DaysofMLCode 

## DAY 27 :
Started learning about Support Vector Machines from @udacity . Made my first SVM using scikit-learn

#100DaysofMLCode 

## DAY 28 :
Learnt about non-linear SVM's. Also learnt about an awesome trick known as the Kernel trick which magically transforms a linear seperator into a non linear seperator.

@udacity 's Intro to ML course is really great and helpful. I recommend other wizards to take this course if you are a beginner.

#100DaysofMLCode 

## DAY 29 :
Went a little deep into decision trees by following @udacity 's Intro to ML course.

#100DaysofMLCode 

## DAY 30 :
I came across a very great article on medium which has all the best Machine Learning Cheat Sheets.

Read about the Scikit-Learn Cheat Sheet.

It is very well formatted and detailed.

I recommend all fellow wizards to read it and share.[CLICK HERE FOR THE LINK](https://l.instagram.com/?u=https%3A%2F%2Flink.medium.com%2FldHHGCiZiR&e=ATOePu4lU9aA199YvYx2CX7lNb-yhJWJNrz8sUVfwmeoWdBr0uzEH7i2vu3kXRWqquzp8jT5Rb9g6V9a)

It is the best thing that I have come across in my Machine Learning Journey.

## DAY 31 :
Revised the T-Shirt Recommendation System that I had made as an assignment for a workshop by @appliedaicourse in my vacations.

I completed the assignment successfully and earned a certificate for it too.

I used libraries like pandas, scikit-learn, keras, tensorflow, matplotlib, etc.

I used techniques like TF-IDF, bag of words, word2vec for text processing and the VGG16 Neural Network for image processing.

#100DaysofMLCode

## DAY 32 :
Again a bit of Decision Tree coding from @udacity 's course.. ..
..
.. 
#100DaysofMLCode

## DAY 33 :
Got a revision of the handwritten digit recognition system that I made by following the book Neural Networks and Deep Learning by Michael Nielsen.

The system only measures accuracy from the given set of training data and test data.

I wasn't able to give my own input to the system. I will soon be updating it on my GitHub.
Any help with it will be great.

I saw an awesome clip on youtube depicting the handwritten digits neural network in an animated format.[See it here](https://www.youtube.com/watch?v=3JQ3hYko51Y&t=38s)

#100DaysofMLCode

## DAY 34-35 :
Applied Google's Deep Dream code by following @sirajraval 's 5th video in his Python for Data Science series.

I first downloaded Google's Inception model, then created a tensorflow session where I imported the inception model graph.

Then I picked a layer out of all the 59 layers and enhanced the image. 
Applied Gradient Ascent on the layer iteratively and got the Deep Dreamed Image.

#100DaysofMLCode

## DAY 36 :
Learnt about Genetic Programming by following @sirajraval 's 6th video in his Python for data science series. 
Genetic programming basically chooses the best ML model to solve a problem and find an optimal solution.

There are basically three steps in genetic programming :
1) Selection
2) Crossover 
3) Mutation

This genetic algorithm will help classify of a given energy is gamma radiation or not.

The program uses tpot (genetic programming library), scikit-learn, numpy and pandas.

#100DaysofMLCode

## DAY 37 :
Not much today. Continuing with @udacity 's Intro to ML course.

#100DaysofMLCode

## DAY 38 :
Back after 2 day's break.... Started with the Math of Intelligence series by @sirajraval .

Learnt about how we can train a model with labelled data by just using y=mx+b. It involved decreasing the error of the prediction by using gradient descent.

The whole series will not use any ML library and will involve making of ML algorithms from scratch.

This is a great way to get into the math of Machine Learning.

#100DaysofMLCode

## DAY 39 :
Sometimes if you are lucky you get a lot of great data and sometimes you don't. 
What do you do when you have less amounts of labelled data ? --> You apply Support Vector Machines (SVM's). ðŸ˜Ž

SVM's are great when you want to perform supervised classification problems.

What SVM's basically do is that the create the best hyperplane that is at equal distance from all classes of data.

Suppose you have data with two features then your hyperplane will be a line, but if your data has 400 features then your hyperplane will be a 399 dimensional figure.

SVM's are normally applied to linear data but can also be applied to non-linear data by using a trick known as the Kernel trick.

SVM's are great for classification but can also be used for regression, finding outliers, etc.

I learnt all of this from @sirajraval 's 2nd video in his The Math of Intelligence series.

#100DaysofMLCode

## DAY 40 :
Done with SVM's on Udacity.

@udacity 's Intro to ML course has really great assignments and they involve lots of challenging questions.

Udacity gave all the assignments in python 2 format so had to do some research to get it all in python 3.

The course didn't go deep into SVM's but I hope to learn it from the Math of Intelligence series by @sirajraval .

#100DaysofMLCode

## DAY 41 :
 The best way to learn something is to express it in an understandable way.

Working on a blog that will explain the T-Shirt Recommendation system. I hope to complete it as soon as possible. But till then you can check the code I have updated on my GitHub. You will find the link to my github [here](https://github.com/TanayY/100DaysofMLCode).

## DAY 42 :
Did research for my blog and updated it further.

## DAY 43 :
Read about the markdown language that is used for the GitHub readme file. Made little changes to the blog.

## DAY 44 :
Found out that @udacity 's Intro to ML course Lesson 15 is related to Bag of Words and TF-IDF, which are text frequency counting concepts. These concepts will help me on the blog. Though I have learnt about these concepts before, the lesson will give me a revision so that I can explain it clearly on my blog.

I am now thinking about completing udacity's course and then working on the blog. So the blog might take time.

Going a little slow this week due to exams.

## DAY 45 :
Read about the concept of TF-IDF and how to apply it using scikit-learn library in python.

## DAY 46 :
Read a paper named "A Few useful things to know about Machine Learning" by Pedro Domingos. The paper focuses on some very useful things that can be used to produce better Machine Learning Algorithms.

Some main points to remember from the paper are :

1. Learning = Representation+Evaluation+Optimisation

2. Generalization is important.

3. Data alone is not enough. 
4. More data --> Cleverer Algorithm.

5. Intuition fails in higher dimensions.

6. Feature engineering is the key to solve the problem.

7. Learn many models.

8. Simplicity does not imply accuracy.

9. Correlation is not the same as causation.

Find the paper here :

https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf

A brief summary of the paper is on this medium article :

https://medium.com/@rupak.thakur/23-deep-learning-papers-to-get-you-started-part-1-308f80d7bba2


## DAY 47 :
Did a project on my own today. I got a dataset of wine contents from kaggle. I divided the data into features and labels. Then I splitted it into training and testing data.

Then applied SVM (Support Vector Machine Classifier), SVC to be specific, using scikit-learn. I got an accuracy of 60%. 60% wasn't good enough, but I think it's great for me on my first self project. I will have to try alternative methods other than SVM to get a higher accuracy.

I did this whole thing on my own, without following any tutorial or video and I think it was really great.

Find the code on my GitHub (link in Bio). Any suggestions as to how to improve the accuracy will be really great.

## DAY 48 :
Today I made a human activity prediction system, which predicts if a person is standing, walking, sleeping,etc. based on the data from a mobile phones sensors

I got the data from @kaggleofficial .
The data set contains all of the data from mobiles sensors as its features and the activity as its label.

I used support vector machines to predict the activity and got an accuracy of 93%, which I think is pretty cool.

Go check out the code on my GitHub.(Link above). 
